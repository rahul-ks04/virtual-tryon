{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Virtual Try-On Inference Orchestrator\n",
                "\n",
                "This notebook orchestrates the full virtual try-on pipeline, from pre-processing (SCHP, DensePose, OpenPose) to flow estimation and final garment warping using FVNT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sys, torch, shutil, numpy as np, math\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from torchvision.ops import deform_conv2d\n",
                "\n",
                "# Setup paths\n",
                "PROJECT_ROOT = \"/content/drive/MyDrive/virtual_tryon_project\"\n",
                "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
                "if f\"{PROJECT_ROOT}/src\" not in sys.path: sys.path.insert(0, f\"{PROJECT_ROOT}/src\")\n",
                "\n",
                "from agnostic_logic import get_agnostic_person\n",
                "\n",
                "print(\"✅ Environment ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. FVNT Setup & Injection\n",
                "Integrating the Zero-Build DCN and loading FEM weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FVNT_DIR = f\"{PROJECT_ROOT}/FVNT\"\n",
                "DCN_FOLDER = os.path.join(FVNT_DIR, \"Deformable\")\n",
                "os.makedirs(DCN_FOLDER, exist_ok=True)\n",
                "with open(os.path.join(DCN_FOLDER, \"__init__.py\"), \"w\") as f: f.write(\"from .modules import DeformConvPack\")\n",
                "\n",
                "modules_py = \"\"\"\n",
                "import torch\n",
                "from torch import nn\n",
                "from torchvision.ops import deform_conv2d\n",
                "import math\n",
                "\n",
                "class DeformConvPack(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding,\n",
                "                 dilation=1, groups=1, deformable_groups=1, im2col_step=64, bias=True, lr_mult=0.1):\n",
                "        super(DeformConvPack, self).__init__()\n",
                "        self.in_channels = in_channels\n",
                "        self.out_channels = out_channels\n",
                "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
                "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
                "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
                "        self.dilation = (dilation, dilation) if isinstance(dilation, int) else dilation\n",
                "        self.groups = groups\n",
                "        self.deformable_groups = deformable_groups\n",
                "        \n",
                "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size))\n",
                "        if bias: self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
                "        else: self.register_parameter('bias', None)\n",
                "            \n",
                "        self.conv_offset = nn.Conv2d(in_channels, \n",
                "                                     deformable_groups * 2 * self.kernel_size[0] * self.kernel_size[1],\n",
                "                                     kernel_size=self.kernel_size, \n",
                "                                     stride=self.stride, \n",
                "                                     padding=self.padding)\n",
                "        self.reset_parameters()\n",
                "\n",
                "    def reset_parameters(self):\n",
                "        n = self.in_channels\n",
                "        for k in self.kernel_size: n *= k\n",
                "        stdv = 1. / math.sqrt(n)\n",
                "        self.weight.data.uniform_(-stdv, stdv)\n",
                "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
                "        self.conv_offset.weight.data.zero_()\n",
                "        self.conv_offset.bias.data.zero_()\n",
                "\n",
                "    def forward(self, x):\n",
                "        offset = self.conv_offset(x)\n",
                "        return deform_conv2d(x, offset, self.weight, self.bias, \n",
                "                             stride=self.stride, padding=self.padding, dilation=self.dilation)\n",
                "\"\"\"\n",
                "with open(os.path.join(DCN_FOLDER, \"modules.py\"), \"w\") as f: f.write(modules_py)\n",
                "\n",
                "if FVNT_DIR not in sys.path: sys.path.insert(0, FVNT_DIR)\n",
                "from mine.network_stage_2_mine_x2_resflow import Stage_2_generator\n",
                "print(\"✅ DCN Injected & Generator Imported\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model & Helpers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "fem = Stage_2_generator(20).to(device)\n",
                "ckpt_path = f\"{PROJECT_ROOT}/checkpoints/stage2_model.pth\"\n",
                "if os.path.exists(ckpt_path):\n",
                "    ckpt = torch.load(ckpt_path, map_location=device)\n",
                "    fem.load_state_dict(ckpt['G'] if 'G' in ckpt else ckpt)\n",
                "    fem.eval()\n",
                "    print(\"✅ FEM Loaded\")\n",
                "\n",
                "def warp_high_res(img_t, low_res_flow):\n",
                "    B, _, H_hr, W_hr = img_t.shape\n",
                "    _, _, H_lr, W_lr = low_res_flow.shape\n",
                "    flow_hr = torch.nn.functional.interpolate(low_res_flow, size=(H_hr, W_hr), mode='bilinear', align_corners=True)\n",
                "    flow_hr[:, 0] = flow_hr[:, 0] * (W_hr / W_lr)\n",
                "    flow_hr[:, 1] = flow_hr[:, 1] * (H_hr / H_lr)\n",
                "    gx = torch.arange(W_hr, device=device).view(1,-1).repeat(H_hr,1).view(1,1,H_hr,W_hr).expand(B,-1,-1,-1)\n",
                "    gy = torch.arange(H_hr, device=device).view(-1,1).repeat(1,W_hr).view(1,1,H_hr,W_hr).expand(B,-1,-1,-1)\n",
                "    grid = torch.cat([gx, gy], 1).float() + flow_hr\n",
                "    grid[:, 0] = 2.0 * grid[:, 0] / max(W_hr - 1, 1) - 1.0\n",
                "    grid[:, 1] = 2.0 * grid[:, 1] / max(H_hr - 1, 1) - 1.0\n",
                "    return torch.nn.functional.grid_sample(img_t, grid.permute(0, 2, 3, 1), align_corners=True)\n",
                "\n",
                "def prep_tensor(img, is_parsing=False):\n",
                "    W_MODEL, H_MODEL = 192, 256\n",
                "    img_resized = img.resize((W_MODEL, H_MODEL), Image.NEAREST if is_parsing else Image.BILINEAR)\n",
                "    if is_parsing:\n",
                "        lbl = np.array(img_resized)\n",
                "        out = torch.zeros(20, H_MODEL, W_MODEL)\n",
                "        for i in [4, 5, 6, 7]: out[i] = torch.from_numpy((lbl == i).astype(np.float32))\n",
                "        return out.unsqueeze(0)\n",
                "    else:\n",
                "        return (torch.from_numpy(np.array(img_resized)).permute(2,0,1).float().unsqueeze(0)/127.5-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pipeline Execution\n",
                "Running pre-processors, agnostic generation, and final warping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_pipeline(person_id, cloth_id):\n",
                "    # 1. Paths\n",
                "    person_img_path = f\"{PROJECT_ROOT}/data/viton_hd/test/image/{person_id}\"\n",
                "    cloth_img_path = f\"{PROJECT_ROOT}/data/viton_hd/test/cloth/{cloth_id}\"\n",
                "    \n",
                "    # 2. Run Pre-processors (Simulated here, in practice use !python scripts)\n",
                "    parse_path = f\"{PROJECT_ROOT}/data/viton_hd/test/image-parse-v3/{person_id.replace('.jpg','.png')}\"\n",
                "    \n",
                "    # 3. Generate Agnostic\n",
                "    agnostic_img, agnostic_parse = get_agnostic_person(person_img_path, parse_path)\n",
                "    \n",
                "    # 4. FEM Inference\n",
                "    input_1 = prep_tensor(agnostic_parse, is_parsing=True).to(device)\n",
                "    cloth_mask = Image.open(f\"{PROJECT_ROOT}/data/viton_hd/test/cloth-mask/{cloth_id}\")\n",
                "    input_2 = torch.zeros(1, 20, 256, 192).to(device)\n",
                "    input_2[0, 5] = torch.from_numpy((np.array(cloth_mask.resize((192, 256))) >= 128).astype(np.float32))\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        flow_list, _ = fem(input_1, input_2)\n",
                "    low_res_flow = flow_list[-1]\n",
                "    \n",
                "    # 5. High-Res Warping\n",
                "    cloth_hd = Image.open(cloth_img_path).resize((768, 1024))\n",
                "    cloth_hd_t = (torch.from_numpy(np.array(cloth_hd)).permute(2,0,1).float().unsqueeze(0)/127.5-1).to(device)\n",
                "    warped_hd = warp_high_res(cloth_hd_t, low_res_flow)\n",
                "    \n",
                "    # Convert for display\n",
                "    res = ((warped_hd[0].permute(1,2,0).cpu().numpy()+1)*0.5).clip(0,1)\n",
                "    plt.imshow(res); plt.axis('off'); plt.show()\n",
                "    \n",
                "    return res"
            ]
        }
    ]
}